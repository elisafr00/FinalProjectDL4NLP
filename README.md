# FinalProjectDL4NLP
Final Project for the DL4NLP course from HAP-LAP/EMLCT Master (UPV/EHU). 

This project investigates three different methods for fine-tuning LLMs pre-trained models.
The LongLoRA technique, an extension of the LoRA method, shows superior efficiency and reduced computational and time costs over the other two methods, full fine-tuning and LoRA. In order to demonstrate this, three experiments are designed and proposed, to compare the techniqueâ€™s performance on the perplexity metric
